{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение и особенности LightGBM\n",
    "\n",
    "__`LightGBM`__ - это фреймворк с реализацией градиетного бустинга, построенного на древовидных алгоритмах обучения. Основные приемущества данного фреймворка:\n",
    "\n",
    "* быстрая скорость обучения и более высокая эффективность\n",
    "* более низкая потребность в памяти\n",
    "* лучшее качество\n",
    "* поддержка параллельного обучения и обучения на GPU\n",
    "* способность работы с крупномасштабными данными\n",
    "\n",
    "## Оптимизация скорости и использования памяти\n",
    "\n",
    "Многие реализации градиентного бустинга используют алгоритмы предварительной сортировки значений признаков (например, так работает `XGBoost`) для построения разбиения дерева решений. Это простое решение, но его нелегко оптимизировать.\n",
    "\n",
    "`LightGBM` использует __алгоритмы на основе гистограмм__, которые объединяют непрерывные значения признаков (атрибутов) в отдельные интервалы (бины) гистограммы. Это значительно ускоряет обучение и снижает использование памяти. К преимуществам алгоритмов на основе гистограмм можно отнести следующее:\n",
    "\n",
    "1. снижена стоимость расчета gain'а для каждого сплита\n",
    " * алгоритмы на основе предварительной сортировки имеют временную сложность __`O(#data)`__\n",
    " * алгоритм на основе гистограммы имеет временную сложность __`O(#bins)`__, при этом __#bins__ значительно меньше __#data__\n",
    "\n",
    "\n",
    "2. использование разности гистограмм для дальнейшего ускорения процесс построения дерева\n",
    " * чтобы получить гистограммы одного листа в бинарном дереве, используется вычитание гистограмма родительской вершины и одной из производных вершин\n",
    " * строится гистограмма только для того листа, который имеет меньше __#data__, после чего можно получить гистограмму соседней вершины, путем вычитания гистограммы, это несложная операция, ее сложность __`O(#bins)`__.\n",
    "\n",
    "\n",
    "3. Непрерывные значения заменяеются дискретными бинами. Если __#bins__ мало, то можно использовать тип данных, который занимает мало место в памяти, например, `uint8` для хранения обучающих данных. Кроме того, неот необходимости хранить дополнительную информацию с предварительно отсортированными значениями признаков.\n",
    "\n",
    "## Оптимизация качества работы алгоритма\n",
    "\n",
    "Большая часть алгоритмов, основанных на решающих деревьях, основаны на __Level-wise__ архитектуре деревьев, дерево строится по уровням, как показано на рисунке (на каждой итерации, производятся 2 дочерние вершины):\n",
    "\n",
    "<img src=\"images/lightgbm_trees_1.png\" width=500 height=400 />\n",
    "\n",
    "__`LightGBM`__ строить деревья по листьям. Он выбирает для роста лист с максимальным приростом функции потерь. Удерживая фиксированное значение __#leaf__, алгоритмы построения деревьев на основе __Leaf-wise__ архитектуры, как правило, приводит к меньшему значению функции потерь, чем алгоритмы, основанные на __Level-wise__ архитектуре.\n",
    "\n",
    "<img src=\"images/lightgbm_trees_2.png\" width=500 height=400 />\n",
    "\n",
    "## Оптимальное разбиения для категориальных признаков\n",
    "\n",
    "Обычно, категориальные признаки обрабатывают с помощью `One-Hot-Encoding`, но этот подход неоптимальный для решающих деревьев. Например, для категориальных признаков с большим количеством уникальных значений, дерево, построенное на основе `One-Hot-Encoding` признаков, имеет тенденцию быть несбалансированным и должно быть очень глубоким для достижения хорошей точности.\n",
    "\n",
    "Вместо `One-Hot-Encoding`, более оптимальным подходом к разбиению вершины по категориальному признаку, является разделение категориального признака на 2 подмножества. Если признак имеет `k` уникальных значений, то существует $2^{(k-1)} - 1$  возможных разбиений. В __`LightGBM`__ используется подход, который требует __O(klog(k))__ операций для поиска оптимального разбиения. Его основная идея состоит в том, чтобы отсортировать категории в соответствии со значением функции потерь для при каждом возможном разбиении. __`LightGBM`__ сортирует гистограмму категориального признака в соответствии с накопленными значениями `sum_gradient / sum_hessian`, и находит оптимальное разделение на отсортированной гистограмме.\n",
    "\n",
    "## GOSS - случайный отбор на основе градиентов\n",
    "\n",
    "__GOSS__ - Gradient Based One Side Sampling\n",
    "\n",
    "Наблюдения с маленькими градиентами - наблюдения, на которых алгоритм работает хорошо, а наблюдения с большими градиентами - наблюдения, где алгоритм работает неуверенно и серьезно ошибается. Существует наивный подход к даунсэмплингу, суть метода в том, чтобы отбросить наблюдения с небольшими градиентами, оставив только примеры с большими градиентами. __НО!__ Такой подход изменит распределение данных. \n",
    "\n",
    "Несмотря на то, что в градиетном бустинге наблюдения не имеют весов, можно заметить, что наблюдения с разными градиентами вносят различный вклад в процесс вычисления информационного выигрыша: объекты с большим градиетном вносят больший вклад в информационный выигрыш. Поэтому, чтобы сохранить точность оценки онформационного выигрыша, необходимо сохранять наблюдения с большими градиентами, то есть брать наблюдения с градиентами выше заранее заданного порога или ориентироваться на верхние процентили, и случайным образом удалять наблюдения с маленьким градиентом. Такой подходит приводит к более точной оценке выигрыша по сравнению с равномерной случайной выборкой.\n",
    "\n",
    "__Алгоритм GOSS__:\n",
    "* отсортировать наблюдения по значению градиента по убыванию;\n",
    "* выбрать верхние экземпляры `a * 100%` с наибольшим значением градиента;\n",
    "* произвольно выбрать `b * 100%` наблюдения из оставшихся данных;\n",
    "* без пунтка 3, количество объектов, имеющих небольшой градиент, было бы равно `1 - a`. Для сохранения исходного распределения, `LightGBM` усиливает вклад наблюдений, имеющих небольшие градиенты, на константу, равную `(1 - a) / b`, чтобы придать большие вес наблюдениям с большим градиентом, но не сильно изменить исходное распределение в данных.\n",
    "\n",
    "## EFB - связывание взаимоисключающих признаков\n",
    "\n",
    "__EFB__ - Exclusive Feature Bundling\n",
    "\n",
    "Давай вспомним, что для построения гистограммы требуется __O(#data)__, причем __`#data = #rows * #cols`__. Если мы сможем уменьшить выборку __#cols__ (то же самое, что и __#features__), мы значительно ускорим процесс построение дерева. В __`LightGBM`__ это достигается за счет объединения признаков. Обычно, мы работаем с данными большой размерности, и, как правило, признаковое пространство является довольно сильно разреженным, что дает нам возможность скоратить количество признаков без больших потерь в качестве. В разреженном пространстве, многие признаки уникальны, то есть редко принимают ненулевые значения одновременно. В качестве примера можно рассмотреть `One-Hot-Encoding` признаки, такие взаимоисключающие признаки можно связывать без потери качества. __`LightGBM`__ безопасно идентифицирует такие признаки и объединяет их в один признак, таким образом сложность построения гистограммы снижается до __O(#rows * #bundles)__, где `#bundles << #features`.\n",
    "\n",
    "__Алгоритм EFB__:\n",
    "* построить граф с взвешенными ребрами; вес ребра определяется как доля исключительных признаков, которые имеют непересекающиеся ненулевые значения;\n",
    "\n",
    "* отсортировать признаки по количеству ненулевых примеров в порядке убывания;\n",
    "\n",
    "* просмотреть упорядоченный список признаков и отнести признак к сгруппированному набору, если значение метрики не превышает порог, или создадим новый набор, если значение метрики превышает порог.\n",
    "\n",
    "## Как параллелится градиетный бустинг? \n",
    "### мой любимый вопрос на собеседовании :)\n",
    "\n",
    "### Традиционный подход\n",
    "Традиционный подход направлен на параллелизацию процесса `поиска наилучшего разбиения` в дереве решений. __Процедура__ традиционного параллелизации выглядит следующим образом:\n",
    "\n",
    "* разделить данные по вертикали (разные ядра / разные workers имеют разные наборы признаков)\n",
    "* worker'ы находят локальную лучшую точку разделения на локальном подмножестве признаков\n",
    "* собрать все лучшие локальные разбиения вместе и выбрать среди лучших локальных разбиений, самое лучшее\n",
    "* остальные worker'ы разбивают данные согласно полученному оптимальному разбиению\n",
    "\n",
    "__Недостатки__ традиционного подхода:\n",
    "\n",
    "* имеет накладные расходы на вычисления, поскольку не может ускорить разбиение вершины, временная сложность разбиения равна __O(#data)__. Таким образом, прирост скорости не очень большой при большом __#data__.\n",
    "\n",
    "### Feature Parallel\n",
    "\n",
    "Поскольку традиционный `Feature Parallel` медленно работает при большом __#data__, внесем небольшое изменение: вместо вертикального разделения данных, каждый worker будет хранить полную обучающую выборку (все данные). Таким образом, __`LightGBM`__ не нуждается в обмене лучшими локальными разбиениями, поскольку каждый worker знает весь набор данных. Процедура `Feature Parallel` в __`LightGBM`__ выглядит следующим образом:\n",
    "\n",
    "* worker'ы находят наилучшую точку разбиения (признак и значение признака) на локальном подмножестве признаков;\n",
    "* сравнить все лучшие локальные разбиения и выбрать оптимальное;\n",
    "* выполнить выбранное разбиение дерева;\n",
    "\n",
    "Такое подход к параллелизации алгоритма, по-прежнему страдает от накладных расходов на вычисление для сплита, когда __#data__ все еще велик. Поэтому лучше использовать `Data Parallel`.\n",
    "\n",
    "### Traditional Data Parallel\n",
    "\n",
    "Традиционный алгоритм направлен на распапаллеливание всего процесса обучения модели:\n",
    "\n",
    "* разбиваем данные по горизонтали;\n",
    "* worker'ы используют локальные данные для построения локальных гистограмм;\n",
    "* все локальные гистограммы объединяются в глобальную гистограмму;\n",
    "* находим наилучшее разбиение в глобальной гистограмме и используем его для выращивания дерева решений;\n",
    "\n",
    "В __`LightGBM`__, используется объединение не всех локальных гистограмм, а объединяются максимально непохожие друг на друга гистограммы, в идеале, объединяются только не пересекающиеся гистограммы. После чего worker'ы находят наилучшее локальное разбиение на объединенных локальных гистограммах и выбирается оптимальное разбиение (глобальное). Кроме того, выше, мы обсуждали, что мы можем получать гистограммы путем вычитания гистограммы потомка из гистограммы предка. Таким образом, мы еще сильнее можем ускорить `Data Parallel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример EFB\n",
    "\n",
    "Попробуем понять интуицию объединения признаков на примере. Перед эти еще раз очертим основную идею метода: EFB объединяет признаки, чтобы упростить обучение. Для того, что объединение признаков было обратимым, будем хранить уникальные признаки в разных бинах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_bundle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_bundle\n",
       "0          0          2               6\n",
       "1          0          1               5\n",
       "2          0          2               6\n",
       "3          1          0               1\n",
       "4          2          0               2\n",
       "5          3          0               3\n",
       "6          4          0               4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efb_data = pd.DataFrame({\n",
    "    \"feature_1\": [0, 0, 0, 1, 2, 3, 4],\n",
    "    \"feature_2\": [2, 1, 2, 0, 0, 0, 0],\n",
    "    \"feature_bundle\": [6, 5, 6, 1, 2, 3, 4]\n",
    "})\n",
    "efb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном примере, видно, что признаки `feature_1` и  `feature_2` исключают друг друга. Чтобы добиться неперекрывающихся сегментов, добавим размер пакета для `feature_1` к `feature_2`. Это гарантирует, что ненулевые примеры объединенного признака `feature_1 & feature_2` находятся в разных сегментах. В признаке `feature_bundle` значения `1-4` содержат ненулевые примеры для `feature_1`, а значения `5-6` содержат ненулевые значения признака `feature_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape = 200000 rows, 202 cols\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    \"data/train.csv\"\n",
    ")\n",
    "print(\"data.shape = {} rows, {} cols\".format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape = 200000 rows, 200 cols\n"
     ]
    }
   ],
   "source": [
    "target = data[\"target\"]\n",
    "data = data.drop([\"ID_code\", \"target\"], axis=1)\n",
    "print(\"data.shape = {} rows, {} cols\".format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = 160000 rows, 200 cols\n",
      "x_valid.shape = 40000 rows, 200 cols\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid = train_test_split(\n",
    "    data, train_size=0.8, random_state=1\n",
    ")\n",
    "y_train, y_valid = train_test_split(\n",
    "    target, train_size=0.8, random_state=1\n",
    ")\n",
    "print(\"x_train.shape = {} rows, {} cols\".format(*x_train.shape))\n",
    "print(\"x_valid.shape = {} rows, {} cols\".format(*x_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"n_estimators\": 200,\n",
    "    \"n_jobs\": 6,\n",
    "    \"seed\": 27\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttraining's auc: 0.716713\tvalid_1's auc: 0.694117\n",
      "[20]\ttraining's auc: 0.729973\tvalid_1's auc: 0.707004\n",
      "[30]\ttraining's auc: 0.741827\tvalid_1's auc: 0.71714\n",
      "[40]\ttraining's auc: 0.753216\tvalid_1's auc: 0.72623\n",
      "[50]\ttraining's auc: 0.762935\tvalid_1's auc: 0.734123\n",
      "[60]\ttraining's auc: 0.774779\tvalid_1's auc: 0.7445\n",
      "[70]\ttraining's auc: 0.784419\tvalid_1's auc: 0.753455\n",
      "[80]\ttraining's auc: 0.793025\tvalid_1's auc: 0.761322\n",
      "[90]\ttraining's auc: 0.800332\tvalid_1's auc: 0.768126\n",
      "[100]\ttraining's auc: 0.80644\tvalid_1's auc: 0.77331\n",
      "[110]\ttraining's auc: 0.812058\tvalid_1's auc: 0.777985\n",
      "[120]\ttraining's auc: 0.81761\tvalid_1's auc: 0.78277\n",
      "[130]\ttraining's auc: 0.822393\tvalid_1's auc: 0.786508\n",
      "[140]\ttraining's auc: 0.826481\tvalid_1's auc: 0.790199\n",
      "[150]\ttraining's auc: 0.830668\tvalid_1's auc: 0.79387\n",
      "[160]\ttraining's auc: 0.834404\tvalid_1's auc: 0.7968\n",
      "[170]\ttraining's auc: 0.838368\tvalid_1's auc: 0.799991\n",
      "[180]\ttraining's auc: 0.841729\tvalid_1's auc: 0.802885\n",
      "[190]\ttraining's auc: 0.84485\tvalid_1's auc: 0.805469\n",
      "[200]\ttraining's auc: 0.847882\tvalid_1's auc: 0.807926\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's auc: 0.847882\tvalid_1's auc: 0.807926\n"
     ]
    }
   ],
   "source": [
    "dtrain = lgb.Dataset(\n",
    "    data=x_train, label=y_train\n",
    ")\n",
    "dvalid = lgb.Dataset(\n",
    "    data=x_valid, label=y_valid\n",
    ")\n",
    "\n",
    "model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=dtrain,\n",
    "    num_boost_round=200,\n",
    "    valid_sets=[dtrain, dvalid],\n",
    "    categorical_feature=\"auto\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttraining's auc: 0.716713\tvalid_1's auc: 0.694117\n",
      "[20]\ttraining's auc: 0.729973\tvalid_1's auc: 0.707004\n",
      "[30]\ttraining's auc: 0.741827\tvalid_1's auc: 0.71714\n",
      "[40]\ttraining's auc: 0.753216\tvalid_1's auc: 0.72623\n",
      "[50]\ttraining's auc: 0.762935\tvalid_1's auc: 0.734123\n",
      "[60]\ttraining's auc: 0.774779\tvalid_1's auc: 0.7445\n",
      "[70]\ttraining's auc: 0.784419\tvalid_1's auc: 0.753455\n",
      "[80]\ttraining's auc: 0.793025\tvalid_1's auc: 0.761322\n",
      "[90]\ttraining's auc: 0.800332\tvalid_1's auc: 0.768126\n",
      "[100]\ttraining's auc: 0.80644\tvalid_1's auc: 0.77331\n",
      "[110]\ttraining's auc: 0.813384\tvalid_1's auc: 0.779508\n",
      "[120]\ttraining's auc: 0.819688\tvalid_1's auc: 0.785428\n",
      "[130]\ttraining's auc: 0.825127\tvalid_1's auc: 0.790613\n",
      "[140]\ttraining's auc: 0.830092\tvalid_1's auc: 0.795163\n",
      "[150]\ttraining's auc: 0.834767\tvalid_1's auc: 0.799409\n",
      "[160]\ttraining's auc: 0.838595\tvalid_1's auc: 0.802016\n",
      "[170]\ttraining's auc: 0.842105\tvalid_1's auc: 0.804969\n",
      "[180]\ttraining's auc: 0.84568\tvalid_1's auc: 0.807847\n",
      "[190]\ttraining's auc: 0.848865\tvalid_1's auc: 0.810467\n",
      "[200]\ttraining's auc: 0.851873\tvalid_1's auc: 0.813225\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's auc: 0.851873\tvalid_1's auc: 0.813225\n"
     ]
    }
   ],
   "source": [
    "params[\"boosting_type\"] = \"goss\"\n",
    "\n",
    "model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=dtrain,\n",
    "    num_boost_round=200,\n",
    "    valid_sets=[dtrain, dvalid],\n",
    "    categorical_feature=\"auto\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a17582007/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:502: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tcv_agg's auc: 0.69654 + 0.00325503\n",
      "[20]\tcv_agg's auc: 0.711838 + 0.00368813\n",
      "[30]\tcv_agg's auc: 0.722289 + 0.00442268\n",
      "[40]\tcv_agg's auc: 0.731624 + 0.00481065\n",
      "[50]\tcv_agg's auc: 0.740605 + 0.00457461\n",
      "[60]\tcv_agg's auc: 0.750053 + 0.00390583\n",
      "[70]\tcv_agg's auc: 0.758772 + 0.00325518\n",
      "[80]\tcv_agg's auc: 0.765526 + 0.00264697\n",
      "[90]\tcv_agg's auc: 0.77133 + 0.00212234\n",
      "[100]\tcv_agg's auc: 0.776675 + 0.00173496\n",
      "[110]\tcv_agg's auc: 0.782948 + 0.00138662\n",
      "[120]\tcv_agg's auc: 0.788478 + 0.00168213\n",
      "[130]\tcv_agg's auc: 0.793202 + 0.00138722\n",
      "[140]\tcv_agg's auc: 0.797387 + 0.00151427\n",
      "[150]\tcv_agg's auc: 0.801 + 0.00142765\n",
      "[160]\tcv_agg's auc: 0.804358 + 0.00156804\n",
      "[170]\tcv_agg's auc: 0.807733 + 0.00162715\n",
      "[180]\tcv_agg's auc: 0.810735 + 0.00189044\n",
      "[190]\tcv_agg's auc: 0.813285 + 0.00188992\n",
      "[200]\tcv_agg's auc: 0.815868 + 0.00198339\n"
     ]
    }
   ],
   "source": [
    "cv_result = lgb.cv(\n",
    "    params=params,\n",
    "    train_set=dtrain,\n",
    "    num_boost_round=200,\n",
    "    categorical_feature=\"auto\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=10,\n",
    "    stratified=True,\n",
    "    shuffle=True,\n",
    "    nfold=5, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Sklearn-API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 25 rounds\n",
      "[10]\ttraining's auc: 0.716713\tvalid_1's auc: 0.694117\n",
      "[20]\ttraining's auc: 0.729973\tvalid_1's auc: 0.707004\n",
      "[30]\ttraining's auc: 0.741827\tvalid_1's auc: 0.71714\n",
      "[40]\ttraining's auc: 0.753216\tvalid_1's auc: 0.72623\n",
      "[50]\ttraining's auc: 0.762935\tvalid_1's auc: 0.734123\n",
      "[60]\ttraining's auc: 0.774779\tvalid_1's auc: 0.7445\n",
      "[70]\ttraining's auc: 0.784419\tvalid_1's auc: 0.753455\n",
      "[80]\ttraining's auc: 0.793025\tvalid_1's auc: 0.761322\n",
      "[90]\ttraining's auc: 0.800332\tvalid_1's auc: 0.768126\n",
      "[100]\ttraining's auc: 0.80644\tvalid_1's auc: 0.77331\n",
      "[110]\ttraining's auc: 0.813384\tvalid_1's auc: 0.779508\n",
      "[120]\ttraining's auc: 0.819688\tvalid_1's auc: 0.785428\n",
      "[130]\ttraining's auc: 0.825127\tvalid_1's auc: 0.790613\n",
      "[140]\ttraining's auc: 0.830092\tvalid_1's auc: 0.795163\n",
      "[150]\ttraining's auc: 0.834767\tvalid_1's auc: 0.799409\n",
      "[160]\ttraining's auc: 0.838595\tvalid_1's auc: 0.802016\n",
      "[170]\ttraining's auc: 0.842105\tvalid_1's auc: 0.804969\n",
      "[180]\ttraining's auc: 0.84568\tvalid_1's auc: 0.807847\n",
      "[190]\ttraining's auc: 0.848865\tvalid_1's auc: 0.810467\n",
      "[200]\ttraining's auc: 0.851873\tvalid_1's auc: 0.813225\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's auc: 0.851873\tvalid_1's auc: 0.813225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='goss', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.01, max_depth=-1,\n",
       "               metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
       "               min_split_gain=0.0, n_estimators=200, n_jobs=6, num_leaves=31,\n",
       "               objective='binary', random_state=None, reg_alpha=0.0,\n",
       "               reg_lambda=0.0, seed=27, silent=True, subsample=1.0,\n",
       "               subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier(**params)\n",
    "model.fit(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    eval_set=[(x_train, y_train), (x_valid, y_valid)],\n",
    "    early_stopping_rounds=25,\n",
    "    eval_metric=\"auc\",\n",
    "    verbose=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
