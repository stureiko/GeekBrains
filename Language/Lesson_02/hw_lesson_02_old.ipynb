{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f62377",
   "metadata": {},
   "source": [
    "1. Создайте мешок слов с помощью ``sklearn.feature_extraction.text.CountVectorizer.fit_transform()``. Применим его к ``'tweet_stemmed'`` и ``'tweet_lemmatized'`` отдельно.\n",
    "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью ``max_df``.\n",
    "- Ограничим количество слов, попадающий в мешок, с помощью ``max_features = 1000``.\n",
    "- Исключим стоп-слова с помощью ``stop_words='english'``.\n",
    "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью ``CountVectorizer.get_feature_names()``.\n",
    "2. Создайте мешок слов с помощью ``sklearn.feature_extraction.text.TfidfVectorizer.fit_transform()``. Применим его к ``'tweet_stemmed'`` и ``'tweet_lemmatized'`` отдельно.\n",
    "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью ``max_df``.\n",
    "- Ограничим количество слов, попадающий в мешок, с помощью ``max_features = 1000``.\n",
    "- Исключим стоп-слова с помощью ``stop_words='english'``.\n",
    "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью ``TfidfVectorizer.get_feature_names()``.\n",
    "3. Натренируем ``gensim.models.Word2Vec`` модель на наших данных.\n",
    "- Тренировать будем на токенизированных твитах ``combine_df['tweet_token']``\n",
    "- Установим следующие параметры: ``size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34``.\n",
    "- Используем функцию ``train()`` с параметром ``total_examples`` равным длине ``combine_df['tweet_token']``, количество ``epochs`` установим 20.\n",
    "4. Давайте немного потестируем нашу модель Word2Vec и посмотрим, как она работает. Мы зададим слово ``positive = \"dinner\"``, и модель вытащит из корпуса наиболее похожие слова c помощью функции ``most_similar``. То же самое попробуем со словом ``\"trump\"``.\n",
    "\n",
    "5. Из приведенных выше примеров мы видим, что наша модель word2vec хорошо справляется с поиском наиболее похожих слов для данного слова. Но как она это делает? Она изучила векторы для каждого уникального слова наших данных и использует косинусное сходство, чтобы найти наиболее похожие векторы (слова).\n",
    "    \n",
    "    Давайте проверим векторное представление любого слова из нашего корпуса, например ``\"food\"``.\n",
    "\n",
    "\n",
    "6. Поскольку наши данные содержат твиты, а не только слова, нам придется придумать способ использовать векторы слов из модели word2vec для создания векторного представления всего твита. Существует простое решение этой проблемы, мы можем просто взять среднее значение всех векторов слов, присутствующих в твите. Длина результирующего вектора будет одинаковой, то есть 200. Мы повторим тот же процесс для всех твитов в наших данных и получим их векторы. Теперь у нас есть 200 функций ``word2vec`` для наших данных.\n",
    "\n",
    "    Необходимо создать вектор для каждого твита, взяв среднее значение векторов слов, присутствующих в твите. \n",
    "    \n",
    "    В цикле сделать:  ``vec += model_w2v[word].reshape((1, size))`` и поделить финальный вектор на количество слов в твите.На выходе должен получиться ``wordvec_df.shape = (49159, 200)``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627fb34",
   "metadata": {},
   "source": [
    "# Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05a96c5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T13:07:02.394055Z",
     "start_time": "2022-04-20T13:07:02.384299Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e173268",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3609681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T10:27:55.570032Z",
     "start_time": "2022-04-20T10:27:54.718780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clear_tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>user when father is dysfunctional and is so se...</td>\n",
       "      <td>[user, when, father, is, dysfunctional, and, i...</td>\n",
       "      <td>[user, father, dysfunctional, selfish, drags, ...</td>\n",
       "      <td>[user, father, dysfunct, selfish, drag, kid, d...</td>\n",
       "      <td>[user, father, dysfunct, selfish, drag, kid, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>user user thanks for lyft credit can use cause...</td>\n",
       "      <td>[user, user, thanks, for, lyft, credit, can, u...</td>\n",
       "      <td>[user, user, thanks, lyft, credit, use, cause,...</td>\n",
       "      <td>[user, user, thank, lyft, credit, use, caus, o...</td>\n",
       "      <td>[user, user, thank, lyft, credit, use, caus, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model love take with all the time in ur</td>\n",
       "      <td>[model, love, take, with, all, the, time, in, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0  #model   i love u take with u all the time in ...   \n",
       "4   5    0.0             factsguide: society now    #motivation   \n",
       "\n",
       "                                         clear_tweet  \\\n",
       "0  user when father is dysfunctional and is so se...   \n",
       "1  user user thanks for lyft credit can use cause...   \n",
       "2                                bihday your majesty   \n",
       "3            model love take with all the time in ur   \n",
       "4                  factsguide society now motivation   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [user, when, father, is, dysfunctional, and, i...   \n",
       "1  [user, user, thanks, for, lyft, credit, can, u...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, love, take, with, all, the, time, in, ur]   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [user, father, dysfunctional, selfish, drags, ...   \n",
       "1  [user, user, thanks, lyft, credit, use, cause,...   \n",
       "2                                  [bihday, majesty]   \n",
       "3                      [model, love, take, time, ur]   \n",
       "4                  [factsguide, society, motivation]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [user, father, dysfunct, selfish, drag, kid, d...   \n",
       "1  [user, user, thank, lyft, credit, use, caus, o...   \n",
       "2                                  [bihday, majesti]   \n",
       "3                      [model, love, take, time, ur]   \n",
       "4                        [factsguid, societi, motiv]   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [user, father, dysfunct, selfish, drag, kid, d...  \n",
       "1  [user, user, thank, lyft, credit, use, caus, o...  \n",
       "2                                  [bihday, majesti]  \n",
       "3                      [model, love, take, time, ur]  \n",
       "4                        [factsguid, societi, motiv]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('../Lesson_01/lesson_01_preprocessing.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e3db48",
   "metadata": {},
   "source": [
    "# Задания\n",
    "## Создайте мешок слов \n",
    "с помощью ``sklearn.feature_extraction.text.CountVectorizer.fit_transform()``. Применим его к ``'tweet_stemmed'`` и ``'tweet_lemmatized'`` отдельно.\n",
    "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью ``max_df``.\n",
    "- Ограничим количество слов, попадающий в мешок, с помощью ``max_features = 1000``.\n",
    "- Исключим стоп-слова с помощью ``stop_words='english'``.\n",
    "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью ``CountVectorizer.get_feature_names()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20633077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T10:54:55.447214Z",
     "start_time": "2022-04-20T10:54:55.398851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user father dysfunct selfish drag kid dysfunct run',\n",
       " 'user user thank lyft credit use caus offer wheelchair van pdx disapoint getthank']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_stemmed = [\" \".join(w) for w in df['tweet_stemmed']]\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), \n",
    "                                   analyzer='word', \n",
    "                                   binary=False,\n",
    "                                   max_df=0.9, \n",
    "                                   max_features = 1000, \n",
    "                                   stop_words='english')\n",
    "\n",
    "# Создаем the Bag-of-Words модель\n",
    "bow_tweet_stemmed = count_vectorizer.fit_transform(res_stemmed)\n",
    "\n",
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names_tweet_stemmed = count_vectorizer.get_feature_names()\n",
    "\n",
    "feature_names_tweet_stemmed_df = pd.DataFrame(bow_tweet_stemmed.toarray(), columns = feature_names_tweet_stemmed)\n",
    "feature_names_tweet_stemmed_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c747857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T13:03:00.371921Z",
     "start_time": "2022-04-20T13:03:00.246272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user father dysfunct selfish drag kid dysfunct run',\n",
       " 'user user thank lyft credit use caus offer wheelchair van pdx disapoint getthank']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_lemmatized = [\" \".join(w) for w in df['tweet_lemmatized']]\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), \n",
    "                                   analyzer='word', \n",
    "                                   binary=False,\n",
    "                                   max_df=0.9, \n",
    "                                   max_features = 1000, \n",
    "                                   stop_words='english')\n",
    "\n",
    "# Создаем the Bag-of-Words модель\n",
    "bow_tweet_lemmatized = count_vectorizer.fit_transform(res_lemmatized)\n",
    "\n",
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names_tweet_lemmatized = count_vectorizer.get_feature_names()\n",
    "\n",
    "feature_names_tweet_lemmatized_df = pd.DataFrame(bow_tweet_lemmatized.toarray(), \n",
    "                                                 columns = feature_names_tweet_lemmatized)\n",
    "feature_names_tweet_lemmatized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb5717",
   "metadata": {},
   "source": [
    "## Создайте мешок слов \n",
    "с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform().\n",
    "Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью ``max_df``.\n",
    "- Ограничим количество слов, попадающий в мешок, с помощью ``max_features = 1000``.\n",
    "- Исключим стоп-слова с помощью ``stop_words='english'``.\n",
    "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью ``TfidfVectorizer.get_feature_names()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a87baf4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T13:07:08.956438Z",
     "start_time": "2022-04-20T13:07:06.199137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>ye</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yeah good</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual   ad  adapt  ...  \\\n",
       "0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "1  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "2  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "3  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "4  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "\n",
       "    ye  yeah  yeah good  year  yesterday   yo  yoga  young  youtub   yr  \n",
       "0  0.0   0.0        0.0   0.0        0.0  0.0   0.0    0.0     0.0  0.0  \n",
       "1  0.0   0.0        0.0   0.0        0.0  0.0   0.0    0.0     0.0  0.0  \n",
       "2  0.0   0.0        0.0   0.0        0.0  0.0   0.0    0.0     0.0  0.0  \n",
       "3  0.0   0.0        0.0   0.0        0.0  0.0   0.0    0.0     0.0  0.0  \n",
       "4  0.0   0.0        0.0   0.0        0.0  0.0   0.0    0.0     0.0  0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_stemmed = [\" \".join(w) for w in df['tweet_stemmed']]\n",
    "\n",
    "count_vectorizer = TfidfVectorizer(ngram_range=(1, 2), \n",
    "                                   analyzer='word', \n",
    "                                   binary=False,\n",
    "                                   max_df=0.9, \n",
    "                                   max_features = 1000, \n",
    "                                   stop_words='english')\n",
    "\n",
    "# Создаем the Bag-of-Words модель\n",
    "bow_tfidf_tweet_stemmed = count_vectorizer.fit_transform(res_stemmed)\n",
    "\n",
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names_tweet_stemmed_tfidf = count_vectorizer.get_feature_names()\n",
    "\n",
    "feature_names_tweet_stemmed_df_tfidf = pd.DataFrame(bow_tfidf_tweet_stemmed.toarray(), \n",
    "                                                    columns = feature_names_tweet_stemmed_tfidf)\n",
    "feature_names_tweet_stemmed_df_tfidf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "84835151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T13:08:44.169700Z",
     "start_time": "2022-04-20T13:08:41.400515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>ye</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yeah good</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual   ad  adapt  ...  \\\n",
       "0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "1  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "2  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "3  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "4  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "\n",
       "    ye  yeah  yeah good  year  yesterday   yo  yoga  young  youtub   yr  \n",
       "0  0.0   0.0        0.0   0.0        0.0  0.0   0.0    0.0     0.0  0.0  \n",
       "1  0.0   0.0        0.0   0.0        0.0  0.0   0.0    0.0     0.0  0.0  \n",
       "2  0.0   0.0        0.0   0.0        0.0  0.0   0.0    0.0     0.0  0.0  \n",
       "3  0.0   0.0        0.0   0.0        0.0  0.0   0.0    0.0     0.0  0.0  \n",
       "4  0.0   0.0        0.0   0.0        0.0  0.0   0.0    0.0     0.0  0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_lemmatized = [\" \".join(w) for w in df['tweet_lemmatized']]\n",
    "\n",
    "count_vectorizer = TfidfVectorizer(ngram_range=(1, 2), \n",
    "                                   analyzer='word', \n",
    "                                   binary=False,\n",
    "                                   max_df=0.9, \n",
    "                                   max_features = 1000, \n",
    "                                   stop_words='english')\n",
    "\n",
    "# Создаем the Bag-of-Words модель\n",
    "bow_tfidf_tweet_lemmatized = count_vectorizer.fit_transform(res_lemmatized)\n",
    "\n",
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names_tweet_lemmatized_tfidf = count_vectorizer.get_feature_names()\n",
    "\n",
    "feature_names_tweet_lemmatized_df_tfidf = pd.DataFrame(bow_tfidf_tweet_lemmatized.toarray(), \n",
    "                                                    columns = feature_names_tweet_lemmatized_tfidf)\n",
    "feature_names_tweet_lemmatized_df_tfidf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e0957",
   "metadata": {},
   "source": [
    "## Натренируем ``gensim.models.Word2Vec`` модель на наших данных.\n",
    "- Тренировать будем на токенизированных твитах ``combine_df['tweet_token']``\n",
    "- Установим следующие параметры: ``size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34``.\n",
    "- Используем функцию ``train()`` с параметром ``total_examples`` равным длине ``combine_df['tweet_token']``, количество ``epochs`` установим 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6704871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
