{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQp0WQbLyQ0j"
      },
      "source": [
        "# Transformer\n",
        "\n",
        "Вспомним про encoder-часть трансформера\n",
        "\n",
        "\n",
        "<img src=\"images/transformer.png\"/>\n",
        "\n",
        "\n",
        "1. Кодируем все слова эмбедингами\n",
        "\n",
        "\n",
        "<img src=\"images/transec1.png\"/>\n",
        "\n",
        "\n",
        "2. Добавляем позиции слов\n",
        "3. Энкодер\n",
        "\n",
        "<img src=\"images/transec2.png\"/>\n",
        "\n",
        "\n",
        "3.1 Первая часть - MultiHead attention. Нужен, чтобы искать взаимосвязь между входными словами\n",
        "\n",
        "<img src=\"images/transec3.png\"/>\n",
        "\n",
        "Например, query - ваш запрос в поисковой строке, key - какое-то описание объекта,с которым взаимодействовали (title страницы), values - драфт, из которого вы выбираете.\n",
        "\n",
        "3.2 Результат self-attention шкалирует и преобразуем софтмаксом. В случае с простым текстом получим матрицу распределения \"вероятностей взаимосвязи\" слов\n",
        "\n",
        "Дальше просто много слоев"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHlyT6YhyQ0m"
      },
      "source": [
        "# BERT\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) \n",
        "\n",
        "\n",
        "Идея в основе BERT лежит очень простая: давайте на вход нейросети будем подавать фразы, в которых 15% слов заменим на [MASK], и обучим нейронную сеть предсказывать эти закрытые маской слова.\n",
        "\n",
        "\n",
        "А для того, чтобы нейросеть научилась понимать соотношения между разными предложениями, дополнительно обучим ее предсказывать, является ли вторая фраза логичным продолжением первой. Или это какая-то случайная фраза, не имеющая никакого отношения к первой.\n",
        "\n",
        "\n",
        "Так, для двух предложений: \"Я пошел в магазин.\" и \"И купил там молоко.\", нейросеть должна ответить, что это логично. А если вторая фраза будет \"Карась небо Плутон\", то должна ответить, что это предложение никак не связано с первым. Ниже мы поиграемся с обоими этими режимами работы BERT.\n",
        "\n",
        "\n",
        "Обучив таким образом нейронную сеть на корпусе текстов из Wikipedia и сборнике книг BookCorpus в течении 4 дней на 16 TPU, получили BERT.\n",
        "\n",
        "\n",
        "<img src=\"images/bert.png\"/>\n",
        "\n",
        "\n",
        "Как это делается на практике:\n",
        "\n",
        "\n",
        "<img src=\"images/bertTrain.png\"/>\n",
        "\n",
        "\n",
        "Кодирует 15 процентов слов [MASK] и учим трансформеры + fully connected слои для классификации. Далее, формируем сет, где 50% предложений идут последовательно и 50% рандомно. Учим классификатор (заменяем первый и последний токены предложения, чтобы модель могла их отличить). Учится софтмакс\n",
        "\n",
        "\n",
        "<img src=\"images/bertTrain2.png\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_yHrW7xyQ0n"
      },
      "outputs": [],
      "source": [
        "config_file = 'bert_data/bert_config.json'\n",
        "bert_model = 'bert_data/bert_model.ckpt'\n",
        "vocab_file = 'bert_data/vocab.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk-iyAs6yQ0o",
        "outputId": "25e0dab7-8659-44e9-d4d0-b921bfd74b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 512, 768), ( 91812096    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 512, 768)     1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 512, 768)     0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 512, 768)     393216      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 512, 768)     0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 512, 768)     1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 512, 768)     0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 512, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 512, 768)     0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 512, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 512, 768)     0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 512, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Dense (Dense)               (None, 512, 768)     590592      Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Norm (LayerNormalization)   (None, 512, 768)     1536        MLM-Dense[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Sim (EmbeddingSimilarity)   (None, 512, 119547)  119547      MLM-Norm[0][0]                   \n",
            "                                                                 Embedding-Token[0][1]            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Masked (InputLayer)       [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "MLM (Masked)                    (None, 512, 119547)  0           MLM-Sim[0][0]                    \n",
            "                                                                 Input-Masked[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "NSP (Dense)                     (None, 2)            1538        NSP-Dense[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 178,566,653\n",
            "Trainable params: 178,566,653\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from bert_token import FullTokenizer, printable_text\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "tokenizer = FullTokenizer(vocab_file=vocab_file, do_lower_case=False)\n",
        "model = load_trained_model_from_checkpoint(config_file, bert_model, training=True)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wzm3bsfyQ0p"
      },
      "source": [
        "# Предобученный берт для предсказания пропущенных слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GZ2_VT8yQ0q",
        "outputId": "7d5b5d0a-a0b9-45a0-9bf4-af140fa56c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Сегодня хорошая[MASK]и поэтому собака[MASK].\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "phrase = 'Сегодня хорошая [MASK] и поэтому собака [MASK].'\n",
        "\n",
        "phrase = re.sub(\"\\s*\\[MASK\\]\\s*\", \"[MASK]\", phrase)\n",
        "print(phrase)\n",
        "phrase = phrase.split('[MASK]')\n",
        "tokens = ['[CLS]']\n",
        "for i in range(len(phrase)):\n",
        "    if i == 0:\n",
        "        tokens = tokens + tokenizer.tokenize(phrase[i]) \n",
        "    else:\n",
        "        tokens = tokens + ['[MASK]'] + tokenizer.tokenize(phrase[i]) \n",
        "tokens = tokens + ['[SEP]'] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhDUg0bjyQ0q",
        "outputId": "daa57327-e91d-404c-c5cb-23356779bf0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'Се',\n",
              " '##го',\n",
              " '##дня',\n",
              " 'хор',\n",
              " '##ош',\n",
              " '##ая',\n",
              " '[MASK]',\n",
              " 'и',\n",
              " 'поэтому',\n",
              " 'со',\n",
              " '##бак',\n",
              " '##а',\n",
              " '[MASK]',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIU2fgG2yQ0r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def token_converter(tokens):\n",
        "    token_input = tokenizer.convert_tokens_to_ids(tokens) \n",
        "    token_input = token_input + [0] * (512 - len(token_input))\n",
        "\n",
        "    mask_input = [0]*512\n",
        "    for i in range(len(mask_input)):\n",
        "        if token_input[i] == 103:\n",
        "            mask_input[i] = 1\n",
        "    seg_input = [0]*512\n",
        "\n",
        "    token_input = np.asarray([token_input])\n",
        "    mask_input = np.asarray([mask_input])\n",
        "    seg_input = np.asarray([seg_input])\n",
        "    \n",
        "    return token_input, mask_input, seg_input\n",
        "\n",
        "token_input, mask_input, seg_input = token_converter(tokens)\n",
        "\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[0] \n",
        "predicts = np.argmax(predicts, axis=-1)\n",
        "predicts = predicts[0][:len(tokens)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CQrGCXZyQ0s",
        "outputId": "5c505a60-95fb-4dee-83b7-8580559393f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[',', 'А']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "', А'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = []\n",
        "for i in range(len(mask_input[0])):\n",
        "    if mask_input[0][i] == 1:                       \n",
        "        out.append(predicts[i]) \n",
        "\n",
        "out = tokenizer.convert_ids_to_tokens(out) \n",
        "print(out)\n",
        "out = ' '.join(out)                               \n",
        "out = printable_text(out)            \n",
        "out = out.replace(' ##','') \n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJcG1FNMyQ0s"
      },
      "source": [
        "# ПРедобученный берт для предсказания последовательности 2 фраз"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnF5SQqgyQ0t"
      },
      "outputs": [],
      "source": [
        "tokens_sen_1 = tokenizer.tokenize(\" Я пришел домой\")\n",
        "tokens_sen_2 = tokenizer.tokenize(\"Зеленый попугай\")\n",
        "\n",
        "tokens = ['[CLS]'] + tokens_sen_1 + ['[SEP]'] + tokens_sen_2 + ['[SEP]']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_yjukagyQ0t",
        "outputId": "d90aa343-dbdf-49f9-c998-4ca645ed292a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'Я',\n",
              " 'при',\n",
              " '##шел',\n",
              " 'домой',\n",
              " '[SEP]',\n",
              " 'З',\n",
              " '##елен',\n",
              " '##ый',\n",
              " 'поп',\n",
              " '##уга',\n",
              " '##й',\n",
              " '[SEP]']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opxKKKDPyQ0u"
      },
      "outputs": [],
      "source": [
        "token_input, mask_input, seg_input = token_converter(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18hS49fJyQ0u",
        "outputId": "082f45b4-5feb-4703-82ba-a64db4eff509"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.957909"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicts = model.predict([token_input, seg_input, mask_input])[1] \n",
        "predicts[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjpbNvBeyQ0v"
      },
      "source": [
        "# Fine-tuning bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay2kGlLlyQ0v"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from string import punctuation\n",
        "from stop_words import get_stop_words\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "import re\n",
        "\n",
        "df_train = pd.read_csv(\"data/train.csv\")\n",
        "df_val = pd.read_csv(\"data/val.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LAg9hTnyQ0v",
        "outputId": "5568c9ba-8d96-4fbd-dab3-d2c0b02f26ed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@alisachachka не уезжаааааааай. :(❤ я тоже не ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>RT @epupybobv: Хочется котлету по-киевски. Зап...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>@KarineKurganova @Yess__Boss босапопа есбоса н...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                               text  class\n",
              "0   0  @alisachachka не уезжаааааааай. :(❤ я тоже не ...      0\n",
              "1   1  RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...      1\n",
              "2   2  RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...      0\n",
              "3   3  RT @epupybobv: Хочется котлету по-киевски. Зап...      1\n",
              "4   4  @KarineKurganova @Yess__Boss босапопа есбоса н...      1"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOE6wP-WyQ0w"
      },
      "outputs": [],
      "source": [
        "train_tokens = map(tokenizer.tokenize, df_train['text'])\n",
        "train_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], train_tokens)\n",
        "train_token_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
        "train_token_ids = map(lambda tids: tids + [0] * (512 - len(tids)), train_token_ids)\n",
        "train_token_ids = np.array(list(train_token_ids))\n",
        "\n",
        "test_tokens = map(tokenizer.tokenize, df_val['text'])\n",
        "test_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], test_tokens)\n",
        "test_token_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))\n",
        "\n",
        "test_token_ids = map(lambda tids: tids + [0] * (512 - len(tids)), test_token_ids)\n",
        "test_token_ids = np.array(list(test_token_ids))\n",
        "\n",
        "train_labels_final = df_train['class'].values\n",
        "test_labels_final = df_val['class'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfhONXGCyQ0x"
      },
      "outputs": [],
      "source": [
        "#!pip install bert-for-tf2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43xyBbxRyQ0x",
        "outputId": "e203bae6-dffc-471c-d344-87fc8d186a74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert_model_layer (BertModelL (None, 512, 768)          177261312 \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 393216)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               100663552 \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 277,925,121\n",
            "Trainable params: 277,925,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import bert\n",
        "\n",
        "bert_params = bert.params_from_pretrained_ckpt(\"bert_data\")\n",
        "bert_layer = bert.BertModelLayer.from_params(bert_params)\n",
        "bert_layer.apply_adapter_freeze()\n",
        "    \n",
        "model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(512,), dtype='int32'),\n",
        "        bert_layer,\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "    ])\n",
        "\n",
        "model.build(input_shape=(None, 512))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.Adam(lr=0.00001), metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKQY0F_QyQ0y",
        "outputId": "5c7431db-183c-4f53-8db1-cbeb4a715967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "        train_token_ids,\n",
        "        train_labels_final,\n",
        "        epochs=5,\n",
        "        validation_data=(test_token_ids, test_labels_final),\n",
        "        verbose=1\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElalVBCbyQ0y"
      },
      "outputs": [],
      "source": [
        "sw = set(get_stop_words(\"ru\"))\n",
        "exclude = set(punctuation)\n",
        "morpher = MorphAnalyzer()\n",
        "\n",
        "def preprocess_text(txt):\n",
        "    txt = str(txt)\n",
        "    txt = \"\".join(c for c in txt if c not in exclude)\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(\"\\sне\", \"не\", txt)\n",
        "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
        "    return \" \".join(txt)\n",
        "\n",
        "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
        "df_val['text'] = df_val['text'].apply(preprocess_text)\n",
        "df_test['text'] = df_test['text'].apply(preprocess_text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "BertOverview.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}