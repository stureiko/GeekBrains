{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6_vcBAmo6k9"
      },
      "source": [
        "# Рекурентные сети для обработки последовательностей\n",
        "\n",
        "Вспомним все, что мы уже знаем про обработку текстов:\n",
        "- Компьютер не понимает текст, поэтому нам нужно его как-то закодировать - представить в виде вектора\n",
        "- В тексте много повторяющихся слов/лишний слов - нужно сделать препроцессинг:\n",
        "    - удалить знаки препинания\n",
        "    - удалить стоп-слова\n",
        "    - привести слова к начальной форме (**стемминг** и **лемматизация**)\n",
        "    - ???\n",
        "    \n",
        "    \n",
        "- После этого мы можем представить наш текст (набор слов) в виде вектора, например, стандартными способами:\n",
        "    - **CounterEncoding** - вектор длины размер нашего словаря\n",
        "        - есть словарь vocab, который можем включать слова, ngram-ы\n",
        "        - каждому документу $doc$ ставим в соответствие вектор $vec\\ :\\ vec[i]=1,\\ если\\ vocab[i]\\ \\in\\ doc$\n",
        "    - **HashingVectorizer** - вектор заранее заданной длины\n",
        "        - каждому документу $doc$ ставим в соответствие вектор $vec\\ :\\ vec[i]=1,\\ если\\ \\exists\\ txt\\ \\in\\ doc:\\ hash(text)\\ =\\ i$\n",
        "    - **TfIdfVectorizer** - вектор длины размер нашего словаря\n",
        "        - есть словарь vocab, который можем включать слова, ngram-ы\n",
        "        - каждому документу $doc$ ставим в соответствие вектор $vec\\ :\\ vec[i]=tf(vocab[i])*idf(vocab[i]),\\ если\\ vocab[i]\\ \\in\\ doc$\n",
        "    \n",
        "        $$ tf(t,\\ d)\\ =\\ \\frac{n_t}{\\sum_kn_k} $$\n",
        "        $$ idf(t,\\ D)\\ =\\ \\log\\frac{|D|}{|\\{d_i\\ \\in\\ D|t\\ \\in\\ D\\}|} $$\n",
        "        \n",
        ", где \n",
        "- $n_t$ - число вхождений слова $t$ в документ, а в знаменателе — общее число слов в данном документе\n",
        "- $|D|$ — число документов в коллекции;\n",
        "- $|\\{d_i\\ \\in\\ D\\mid\\ t\\in d_i\\}|$— число документов из коллекции $D$, в которых встречается $t$ (когда $n_t\\ \\neq\\ 0$).\n",
        "\n",
        "\n",
        "\n",
        "Это база и она работает. Мы изучили более продвинутые подходы: эмбединги и сверточные сети по эмбедингам. Но тут есть проблема: любой текст - это последовательность, ни эмбединги, ни сверточные сети не работают с ним как с последовательностью. Так давайте попробуем придумать архитектуру, которая будет работать с текстом как с последовательностью, двигаясь по эмбедингам и как-то меняя их значения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5RrHMlfo6k_"
      },
      "source": [
        "## Придумаем сами архитектуру, чтобы работать с последовательностями\n",
        "\n",
        "Возьмем перцептрон с входом - эмбедингом слова (пусть пока он фиксированный) и будем пытаться классифицировать каждое слово.\n",
        "\n",
        "Почему классифицировать? потому что это частая задача в обработке языка + это дает возможность генерировать текст (просто классифицируем на кол-во классов = кол-ву слов в словаре).\n",
        "\n",
        "<img src=\"images/Single_layer_perceptron.png\">\n",
        "\n",
        "Какая тут последовательность? никакой, но давайте на вход подавать эмбединг, но в 1 скрытый слой будем добавлять последний скрытый слой предыдущего шага)\n",
        "\n",
        "<img src=\"images/Rnnbr.png\">\n",
        "\n",
        "То есть мы прокидываем информацию с предыдущего шага, а за счет того, что мы все время так стекаем вектора мы получаем то, что информация проходит через текст от начала до конца. Что делать с 1 шагом? -> Добавим вектор из нулей. И вот мы получили первую рекурентную сеть. Чаще её рисуют следующим образом:\n",
        "\n",
        "\n",
        "<img src=\"images/Rnn.png\">\n",
        "\n",
        "Итак, мы придумали простую рекуретную сеть. Последний открытый вопрос как её обучать?\n",
        "\n",
        "Все также, градиентным спуском, нам нужно двигаться во времени и обновлять параметры, поэтому обучение таких сетей занимает очень много времени (вы не можете обновить веса для 1-го токена, пока не посчитаете градиент сквозь время).\n",
        "\n",
        "\n",
        "Что делать, если мы хотим классифицировать текст целиком? оставить только последний выход!\n",
        "\n",
        "<img src=\"images/RnnTasks.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpooDtBoo6lB"
      },
      "outputs": [],
      "source": [
        "# попробуем запрограммировать простую рекурентную сеть. Возьмем датасет с прошлого занятия\n",
        "\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "from stop_words import get_stop_words\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "import re\n",
        "\n",
        "df_train = pd.read_csv(\"data/train.csv\")\n",
        "df_test = pd.read_csv(\"data/test.csv\")\n",
        "df_val = pd.read_csv(\"data/val.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7G2jv7Ro6lC"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfMrjVd6o6lD"
      },
      "outputs": [],
      "source": [
        "sw = set(get_stop_words(\"ru\"))\n",
        "exclude = set(punctuation)\n",
        "morpher = MorphAnalyzer()\n",
        "\n",
        "def preprocess_text(txt):\n",
        "    txt = str(txt)\n",
        "    txt = \"\".join(c for c in txt if c not in exclude)\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(\"\\sне\", \"не\", txt)\n",
        "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
        "    return \" \".join(txt)\n",
        "\n",
        "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
        "df_val['text'] = df_val['text'].apply(preprocess_text)\n",
        "df_test['text'] = df_test['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrIVhJYlo6lD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2XxVPWao6lE"
      },
      "outputs": [],
      "source": [
        "text_corpus_train = df_train['text'].values\n",
        "text_corpus_valid = df_val['text'].values\n",
        "text_corpus_test = df_test['text'].values\n",
        "\n",
        "counts = Counter()\n",
        "for sequence in text_corpus_train:\n",
        "    counts.update(sequence.split())\n",
        "\n",
        "print(\"num_words before:\",len(counts.keys()))\n",
        "for word in list(counts):\n",
        "    if counts[word] < 2:\n",
        "        del counts[word]\n",
        "print(\"num_words after:\",len(counts.keys()))\n",
        "    \n",
        "vocab2index = {\"\":0, \"UNK\":1}\n",
        "words = [\"\", \"UNK\"]\n",
        "for word in counts:\n",
        "    vocab2index[word] = len(words)\n",
        "    words.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6ClV7nio6lE"
      },
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "\n",
        "class TwitterDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, txts, labels, w2index, used_length):\n",
        "        self._txts = txts\n",
        "        self._labels = labels\n",
        "        self._length = used_length\n",
        "        self._w2index = w2index\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._txts)\n",
        "    \n",
        "    @lru_cache(50000)\n",
        "    def encode_sentence(self, txt):\n",
        "        encoded = np.zeros(self._length, dtype=int)\n",
        "        enc1 = np.array([self._w2index.get(word, self._w2index[\"UNK\"]) for word in txt.split()])\n",
        "        length = min(self._length, len(enc1))\n",
        "        encoded[:length] = enc1[:length]\n",
        "        return encoded, length\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        encoded, length = self.encode_sentence(self._txts[index])\n",
        "        return torch.from_numpy(encoded.astype(np.int32)), self._labels[index], length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xs-Ka_co6lF"
      },
      "outputs": [],
      "source": [
        "max([len(i.split()) for i in text_corpus_train])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxSAuc_Co6lG"
      },
      "outputs": [],
      "source": [
        "y_train = df_train['class'].values\n",
        "y_val = df_val['class'].values\n",
        "\n",
        "train_dataset = TwitterDataset(text_corpus_train, y_train, vocab2index, 27)\n",
        "valid_dataset = TwitterDataset(text_corpus_valid, y_val, vocab2index, 27)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                          batch_size=128,\n",
        "                          shuffle=True,\n",
        "                          num_workers=3)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                          batch_size=128,\n",
        "                          shuffle=False,\n",
        "                          num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O38bou70o6lG"
      },
      "outputs": [],
      "source": [
        "class RNNFixedLen(nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        rnn_out, (ht, ct) = self.rnn(x)\n",
        "        return self.linear(rnn_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XE-LnMho6lH"
      },
      "outputs": [],
      "source": [
        "rnn_init = RNNFixedLen(len(vocab2index), 30, 20)\n",
        "optimizer = torch.optim.Adam(rnn_init.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7zupF56o6lH"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm_notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWlo5hTOo6lI"
      },
      "outputs": [],
      "source": [
        "for epoch in tqdm_notebook(range(10)):  \n",
        "    rnn_init.train()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels, lengths = data[0], data[1], data[2]\n",
        "        inputs = inputs.long()\n",
        "        labels = labels.long().view(-1, 1)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = rnn_init(inputs, lengths)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    rnn_init.eval()\n",
        "    loss_accumed = 0\n",
        "    for X, y, lengths in valid_loader:\n",
        "        X = X.long()\n",
        "        y = y.long().view(-1, 1)\n",
        "        output = rnn_init(X, lengths)\n",
        "        loss = criterion(output, y)\n",
        "        loss_accumed += loss\n",
        "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
        "\n",
        "print('Training is finished!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m91E6Wo7o6lI"
      },
      "source": [
        "# Какие проблемы у рекурентных сетей?\n",
        "\n",
        "- затухают градиенты\n",
        "- медленно, нужно всегда дойти до конца\n",
        "\n",
        "Как решить? -> LSTM\n",
        "\n",
        "\n",
        "<img src=\"images/lstm.png\">\n",
        "\n",
        "\n",
        "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "\n",
        "Давайте, кратко посмотрим как это работает:\n",
        "\n",
        "\n",
        "<img src=\"images/LSTMMaths.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzlCNORAo6lJ"
      },
      "outputs": [],
      "source": [
        "class LSTMFixedLen(nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, (ht, ct) = self.lstm(x)\n",
        "        return self.linear(lstm_out)\n",
        "    \n",
        "lstm_init = LSTMFixedLen(len(vocab2index), 30, 20)\n",
        "optimizer = torch.optim.Adam(lstm_init.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qw1Mpj_Co6lJ"
      },
      "outputs": [],
      "source": [
        "for epoch in tqdm_notebook(range(10)):  \n",
        "    lstm_init.train()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels, lengths = data[0], data[1], data[2]\n",
        "        inputs = inputs.long()\n",
        "        labels = labels.long().view(-1, 1)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = lstm_init(inputs, lengths)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    lstm_init.eval()\n",
        "    loss_accumed = 0\n",
        "    for X, y, lengths in valid_loader:\n",
        "        X = X.long()\n",
        "        y = y.long().view(-1, 1)\n",
        "        output = lstm_init(X, lengths)\n",
        "        loss = criterion(output, y)\n",
        "        loss_accumed += loss\n",
        "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
        "\n",
        "print('Training is finished!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2gbd_Oao6lK"
      },
      "source": [
        "# Какие проблемы:\n",
        "\n",
        "- вычислительно сложно -> медленнее\n",
        "- на очень длинных последовательностях все равно затухает градиент\n",
        "\n",
        "\n",
        "Зачем платить больше - уберем некоторые врата (точнее совместим) -> ускоримся, уменьшим число параметров -> GRU\n",
        "\n",
        "\n",
        "<img src=\"images/gru.png\">\n",
        "\n",
        "\n",
        "GRU Math\n",
        "\n",
        "\n",
        "<img src=\"images/GRUMath.png\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtq6UR4co6lK"
      },
      "outputs": [],
      "source": [
        "class GRUFixedLen(nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, (ht, ct) = self.lstm(x)\n",
        "        return self.linear(lstm_out)\n",
        "    \n",
        "gru_init = GRUFixedLen(len(vocab2index), 30, 20)\n",
        "optimizer = torch.optim.Adam(gru_init.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_5BDD5Go6lK"
      },
      "outputs": [],
      "source": [
        "for epoch in tqdm_notebook(range(10)):  \n",
        "    gru_init.train()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels, lengths = data[0], data[1], data[2]\n",
        "        inputs = inputs.long()\n",
        "        labels = labels.long().view(-1, 1)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = gru_init(inputs, lengths)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    gru_init.eval()\n",
        "    loss_accumed = 0\n",
        "    for X, y, lengths in valid_loader:\n",
        "        X = X.long()\n",
        "        y = y.long().view(-1, 1)\n",
        "        output = gru_init(X, lengths)\n",
        "        loss = criterion(output, y)\n",
        "        loss_accumed += loss\n",
        "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
        "\n",
        "print('Training is finished!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siseT32bo6lL"
      },
      "source": [
        "3 подхода:\n",
        "\n",
        "<img src=\"images/RNNCompar.png\">\n",
        "\n",
        "\n",
        "Как регуляризовать?\n",
        "- дропаут\n",
        "- рекурентный дропаут\n",
        "\n",
        "\n",
        "<img src=\"images/Dropouts.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24YgtqKQo6lL"
      },
      "outputs": [],
      "source": [
        "# Можно строить lstm с переменным размером входа:\n",
        "class LSTM_variable_input(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 5)\n",
        "        \n",
        "    def forward(self, x, s):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n",
        "        out_pack, (ht, ct) = self.lstm(x_pack)\n",
        "        out = self.linear(ht[-1])\n",
        "        return out"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Lecture07.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}