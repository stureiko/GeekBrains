{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a4c102",
   "metadata": {},
   "source": [
    "# Задание:\n",
    "1. Самостоятельно разобраться с тем, что такое tfidf (документация https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html и еще - https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "2. -ОК Модифицировать код функции get_user_embedding таким образом, чтобы считалось не среднее (как в примере np.mean), а медиана. Применить такое преобразование к данным, обучить модель прогнозирования оттока и посчитать метрики качества и сохранить их: roc auc, precision/recall/f_score (для 3 последних - подобрать оптимальный порог с помощью precision_recall_curve, как это делалось на уроке)\n",
    "3. -ОК Повторить п.2, но используя уже не медиану, а max\n",
    "4. (опциональное, если очень хочется) Воспользовавшись полученными знаниями из п.1, повторить пункт 2, но уже взвешивая новости по tfidf (подсказка: нужно получить веса-коэффициенты для каждого документа. Не все документы одинаково информативны и несут какой-то положительный сигнал). Подсказка 2 - нужен именно idf, как вес.\n",
    "5. -ОК Сформировать на выходе единую таблицу, сравнивающую качество 3 разных метода получения эмбедингов пользователей: mean, median, max, idf_mean по метрикам roc_auc, precision, recall, f_score\n",
    "6. Сделать самостоятельные выводы и предположения о том, почему тот или ной способ оказался эффективнее остальных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a9bbb9",
   "metadata": {},
   "source": [
    "# Исходный код вебинара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e58208",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/igor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Импорты\n",
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from razdel import tokenize \n",
    "\n",
    "import pymorphy2\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, \\\n",
    "                            precision_score, classification_report, \\\n",
    "                            precision_recall_curve, confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "849a6ec4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Данные\n",
    "news = pd.read_csv(\"articles.csv\")\n",
    "\n",
    "users = pd.read_csv(\"users_articles.csv\")\n",
    "\n",
    "stopword_ru = stopwords.words('russian')\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "with open('stopwords.txt') as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "stopword_ru += additional_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15b3af3",
   "metadata": {
    "code_folding": [
     0,
     27
    ]
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    \n",
    "    #tokens = list(tokenize(text))\n",
    "    #words = [_.text for _ in tokens]\n",
    "    #words = [w for w in words if w not in stopword_ru]\n",
    "    \n",
    "    #return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист отлемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w)>1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords=[i for i in words_lem if not i in stopword_ru] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca37cfd6",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-7ee348d9b386>:15: FutureWarning: Possible nested set at position 39\n",
      "  text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.7 s, sys: 504 ms, total: 42.2 s\n",
      "Wall time: 43.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем очистку текста. Будет долго...\n",
    "news['title'] = news['title'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f00a89",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 47s, sys: 3.73 s, total: 5min 50s\n",
      "Wall time: 6min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем лемматизацию текста. Будет очень долго...\n",
    "news['title'] = news['title'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b88820f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#сформируем список наших текстов, разбив еще и на пробелы\n",
    "texts = [t for t in news['title'].values]\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe824d3f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Запуск обучения\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=25, id2word=common_dictionary)#, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f7fd389",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Save model to disk.\n",
    "temp_file = datapath(\"model.lda\")\n",
    "lda.save(temp_file)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "120640a5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.0942287),\n",
       " (6, 0.22201407),\n",
       " (11, 0.23112985),\n",
       " (14, 0.20597105),\n",
       " (24, 0.22555146)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new corpus, made of previously unseen documents.\n",
    "other_texts = [t for t in news['title'].iloc[:3]]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[2]\n",
    "# print(other_texts[2])\n",
    "lda[unseen_doc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae42f174",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# x=lda.show_topics(num_topics=25, num_words=7,formatted=False)\n",
    "# topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "#Below Code Prints Only Words \n",
    "# for topic,words in topics_words:\n",
    "#     print(\"topic_{}: \".format(topic)+\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b61fb5b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# функция, которая возвращает векторное представление новости\n",
    "def get_lda_vector(text):\n",
    "    \"\"\"функция, которая возвращает векторное представление новости\"\"\"\n",
    "    unseen_doc = common_dictionary.doc2bow(text)\n",
    "    lda_tuple = lda[unseen_doc]\n",
    "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(25):\n",
    "        if i not in not_null_topics:\n",
    "            output_vector.append(0)\n",
    "        else:\n",
    "            output_vector.append(not_null_topics[i])\n",
    "    return np.array(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6d9d8ac",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# вектора наших новостей\n",
    "topic_matrix = pd.DataFrame([get_lda_vector(text) for text in news['title'].values])\n",
    "topic_matrix.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "topic_matrix['doc_id'] = news['doc_id'].values\n",
    "topic_matrix = topic_matrix[['doc_id']+['topic_{}'.format(i) for i in range(25)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "283cea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix[['topic_{}'.format(i) for i in range(25)]].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa695007",
   "metadata": {},
   "source": [
    "## Реализация ДЗ - среднее, max и median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64caa8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция с которой будем работать\n",
    "def get_user_embedding(user_articles_list, met='mean'):\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    if met == 'median':\n",
    "        user_vector = np.median(user_vector, 0)\n",
    "    elif met == 'max':\n",
    "        user_vector = np.max(user_vector, 0)\n",
    "    else:\n",
    "        user_vector = np.mean(user_vector, 0)\n",
    "    return user_vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62a1554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: mean\n",
      "Best Threshold=0.249950, F-Score=0.619, Precision=0.543, Recall=0.718\n",
      "roc_auc_score: 0.9239165067736496\n",
      "* * * * * * * * * * \n",
      "\n",
      "Metrics: max\n",
      "Best Threshold=0.430458, F-Score=0.825, Precision=0.876, Recall=0.780\n",
      "roc_auc_score: 0.9790383161811733\n",
      "* * * * * * * * * * \n",
      "\n",
      "Metrics: median\n",
      "Best Threshold=0.267428, F-Score=0.769, Precision=0.703, Recall=0.849\n",
      "roc_auc_score: 0.9701564044421188\n",
      "* * * * * * * * * * \n"
     ]
    }
   ],
   "source": [
    "metrics = ['mean', 'max', 'median']\n",
    "for m in metrics:\n",
    "    # получим эмбединги для всех пользователей\n",
    "    user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x, m), 1)])\n",
    "    user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "    user_embeddings['uid'] = users['uid'].values\n",
    "    user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
    "\n",
    "    # обучим модель. Загрузим разметку.\n",
    "    target = pd.read_csv(\"users_churn.csv\")\n",
    "    X = pd.merge(user_embeddings, target, 'left')\n",
    "\n",
    "    #разделим данные на train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]], \n",
    "                                                        X['churn'], random_state=0)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    #обучим \n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    #наши прогнозы для тестовой выборки\n",
    "    preds = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    # locate the index of the largest f score\n",
    "    ix = np.argmax(fscore)\n",
    "    print(f'\\nMetrics: {m}')\n",
    "    print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                            fscore[ix],\n",
    "                                                                            precision[ix],\n",
    "                                                                            recall[ix]))\n",
    "    print(f'roc_auc_score: {roc_auc_score(y_test, preds)}')\n",
    "    print('* ' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4d34325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('say \"Calculation complete\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee21cb3",
   "metadata": {},
   "source": [
    "## Не получилось реализовать пункт 4 - не понял как. Прошу разобрать этот момент на вебинаре или подскажите мне лично, если остальные справились."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7adc913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
